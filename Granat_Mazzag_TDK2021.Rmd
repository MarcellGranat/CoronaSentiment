---
title: "Reális képet festenek-e a koronavírusjárvány súlyosságáról az európai sajtó hírei?"
author: "Granát Marcell és Mazzag Bálint"
date: \today
output: 
  pdf_document: 
    fig_caption: yes
    toc: yes
    toc_depth: 4
header-includes:
- \usepackage{fancyhdr}
- \usepackage[hungarian]{babel}
- \usepackage{natbib}
- \pagestyle{fancy}
- \fancyhf{}
- \fancyhead[RE,LO]{\leftmark}
- \fancyfoot[C]{\thepage}
- \usepackage{lscape}
- \usepackage{pdfpages}
- \usepackage{titling}
- \pretitle{\begin{center}\LARGE\includegraphics[width=5cm]{logo.png}\\[\bigskipamount]}
- \posttitle{\end{center}}
editor_options: 
  chunk_output_type: inline
---

\pagebreak

\renewcommand{\abstractname}{Absztrakt}

```{=tex}
\begin{abstract}
Kutatásunkban szótáralapú szövegelemzési módszerekkel megvizsgáljuk, hogy a különböző európai országok sajtótermékeiben megjelenő koronavírusról szóló hírek milyen szentimenttel rendelkeztek az elmúlt egy évben. Továbbá az országok médiumait klaszterekbe soroljuk az alapján, hogy mennyire reflektálta az átadott érzelem a járvány súlyosságát. Ehhez különböző panel- és idősorelemzési módszereket használtunk elemezve mind a fertőzöttek és halálozások számát, mind pedig a gazdasági mutatókat is. Végül Magyarországot vizsgáljuk meg részletesen és megmutatjuk, hogy a járvány első két hulláma között volt-e különbség a szentiment és a súlyosság együttmozgása között: az első hullám hatására enyhült-e a közvélemény az év második felére.
\end{abstract}
```


\listoftables

\listoffigures

\pagebreak

# Bevezetés

```{r setup, include=FALSE, warning=F}
knitr::opts_chunk$set(echo = F, comment = "", warning = F, message = F, cache = T, dev = "cairo_pdf", error = T)
```

```{r packages}
# Set up --------------------------------------------------------------------------------

## Packages ============================================================================= 

library(tidyverse)
library(patchwork)
library(knitr)
library(broom)
library(geofacet)
library(tidytext)
library(tm)
library(wordcloud)
library(lubridate)

```

```{r theme}
## Gg theme =============================================================================

update_geom_defaults("point", list(fill = "cyan4", 
                                   shape = 21, 
                                   color = "black", 
                                   size = 1.4))
update_geom_defaults("line", 
                     list(color = "midnightblue", size = 1.4))

update_geom_defaults("smooth", list(color = "red4", size = 1.4))

update_geom_defaults("density", 
                     list(color = "midnightblue", fill =  "midnightblue",
                          alpha = .3, size = 1.4))

extrafont::loadfonts(device="win")

theme_set(theme_grey() + theme(
  legend.direction = "vertical",
  plot.caption = element_text(family = "serif")
))

```

```{r}
# Data ----------------------------------------------------------------------------------

load("dat_aux.RData")
# This RData contains the articles after the main cleaning process
# To ensure full reproducibility see the attached files at the corresponding
# GitHub Repo: -> https://github.com/MarcellGranat/CoronaSentiment <-

```

# Adatok

## Gépi fordítás

```{r fig.cap="Leggyakrabban előforduló szavak a magyar nyelvű cikkekben a fordítást megelőzően és azt követően.", fig.height=8}
# Automatic translation =================================================================

st_hu <- c(stopwords::stopwords('hungarian'), "is", "ha", "hozzá", "címlapfotó",
           "illusztráció") %>% 
  {ifelse(str_starts(., "új"), NA, .)} %>% 
  na.omit()

ggpubr::ggarrange(
  Hungary_rawtext %>% 
    filter(!str_detect(words, '\\d')) %>% 
    anti_join(data.frame(words = st_hu)) %>% 
    count(words, sort = T) %>% 
    arrange(desc(n)) %>% 
    head(30) %>% 
    mutate(
      words = fct_reorder(words, n)
    ) %>% 
    ggplot() +
    aes(n, words) + 
    geom_vline(xintercept = 0) +
    geom_col(color = 'black', fill = "gray70") +
    labs(title = 'Magyarul', x = 'Előfordulási gyakoriság', y = NULL),
  
  dat_sentiment %>% 
    filter(country == 'HU') %>% 
    filter(!str_detect(words, '\\d')) %>% 
    anti_join(data.frame(words = c(stopwords::stopwords(), "also", "can"))) %>% 
    count(words, value, sort = T) %>%
    arrange(desc(n)) %>%
    head(30) %>% 
    mutate(
      value = case_when(
        value < 0 ~ "Negatív",
        value > 0 ~ "Pozitív", 
        T ~ "Nincs"
      ),
      words = fct_reorder(words, n)
    ) %>% 
    ggplot() +
    aes(n, words, fill = value) + 
    geom_vline(xintercept = 0) +
    geom_col(color = "black") +
    labs(title = 'Fordítást követően', x = 'Előfordulási gyakoriság', y = NULL, 
         fill = "Adott szó szentimentje") +
    scale_fill_manual(values = c('red4', 'gray70', 'green')) + 
    theme(
      legend.position = 'bottom',
      legend.direction = 'horizontal'
    ), common.legend = T
)

```

# Leíró statisztikák

```{r fig.cap="A szentiment alakulása országonként", fig.height=10, fig.width=15, out.extra='angle=90', fig.align ='center', }
# Explore the data ----------------------------------------------------------------------

ggplot(dat_sentiment_daily, aes(date, value)) +
  geom_hline(yintercept = 0, color = "grey20") +
  geom_line(size = .3, color = 'grey50') +
  geom_smooth(size = 1.5, se = F) +
  facet_geo(~ code, grid = mygrid, label = 'name') +
  scale_x_date(limits = c(min(dat_sentiment_daily$date), max(dat_sentiment_daily$date)),
               breaks = c(min(dat_sentiment_daily$date), max(dat_sentiment_daily$date))) +
  labs(y = "Szentiment", x = NULL)

```

```{r fig.cap="Leggyakrabban előforduló pozitív és negatív szentimenttel rendelkező szavak"}
library(reshape2)

dat_sentiment %>% 
  na.omit() %>% 
  mutate(
    sentiment = ifelse(value > 0, "Pozitív", "Negatív")
  ) %>% 
  count(words, sentiment, sort = TRUE) %>%
  acast(words ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red4", "cyan4"),
                   max.words = 100)

```

```{r}
dat_plm <- dat_eco_sent %>% 
  filter(indic == "BS-ESI-I") %>% 
  select(date = time, code = geo, eco = values) %>% 
  merge(dat_sentiment_monthly) %>% 
  merge(dat_unemployment) %>% 
  merge(dat_covid_monthly) %>% 
  mutate(
    t = lubridate::interval(lubridate::ymd('2020-01-01'), date),
    t = lubridate::as.period(t) %/% months(1)
  )

```

```{r}
# Regression tree -----------------------------------------------------------------------

m_tree <- rpart::rpart(data = dat_plm, formula = value ~ .-date-code-n,
                       cp = .01)

rattle::fancyRpartPlot(m_tree, palettes = 'PuRd', sub = NULL)

```

# Modellezés és hipotézis vizsgálat

```{r}
## Initial models =======================================================================

library(plm)

m_panels <- c(
  'value ~  unemployment + cases + death + t + eco + t:death'
) %>% 
  lapply(function(formula) {
    pooling <- plm(eval(formula), data = dat_plm, model = "pooling")
    within <- plm(eval(formula), data = dat_plm, model = "within")
    random <- plm(eval(formula), data = dat_plm, model = "random")
    list(
      tests = c(
        pooltest(pooling, within)$p.value,
        phtest(within, random)$p.value,
        plm::r.squared(within, dfcor = T)),
      model = within,
      OLS = lm(data = dat_plm, formula = eval(paste(formula, "+ code")))
    )
  })

```

```{r}
m_panels[[1]]$tests
```

```{r}
m_panels[[1]]$OLS %>% 
  broom::tidy() %>% 
  filter(!str_detect(term, 'code')) %>% 
  knitr::kable(caption = 'Becsült koeffieciensek a fix hatás modellre')
```

```{r}
m_panels[[1]]$OLS %>% 
  QuantPsyc::lm.beta() %>% 
  broom::tidy() %>% 
  filter(!str_detect(term, 'code'))
```

```{r results='asis'}
merge(
  dat_sentiment %>% 
    filter(country == 'HU') %>% 
    mutate(
      date = ym(paste(as.character(year(date)), as.character(month(date)))),
      words = SnowballC::wordStem(words)
    ) %>% 
    group_by(date, words) %>% 
    summarise(n = n()),
  dat_sentiment %>% 
    filter(country == 'HU') %>% 
    group_by(words) %>% 
    summarise(total = n())) %>% 
  mutate(tfr = n/total, date) %>% 
  filter(total > 40) %>% 
  arrange(desc(tfr)) %>% 
  tibble() %>% 
  {split(., with(., date), drop = TRUE)} %>% 
  lapply(function(x) select(head(x, 10), words)) %>% 
  reduce(cbind) %>% 
  {set_names(., str_sub(as.character(as.Date(1:ncol(.)*29, origin = '2020-01-01')),
                        end = 7))} %>% 
  {
    print(knitr::kable(select(., 1:7), align = rep('c', 7)))
    print(knitr::kable(select(., 8:14), align = rep('c', 7), caption = 'Lesz még'))
  }

```

```{r}
dat_sentiment %>% 
  mutate(
    words = SnowballC::wordStem(words)
  ) %>% 
  filter(!str_detect(words, '\\d')) %>% 
  anti_join(data.frame(words = c(stopwords::stopwords(), "also", "can"))) %>% 
  group_by(country, words) %>% 
  summarise(n = n()) %>% 
  {split(., .$country, drop = TRUE)} %>% 
  lapply(function(x) mutate(x, n = n/sum(x$n))) %>% 
  reduce(rbind) %>% 
  pivot_wider(names_from = country, values_from = n, values_fill = 0) %>% 
  select(-words) %>% 
  cor() %>% 
  data.frame() %>% 
  rownames_to_column(var = 'x') %>% 
  pivot_longer(-1, names_to = 'y') %>% 
  filter(x < y) %>% 
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = 'black') +
  scale_fill_viridis_b(direction = -1) +
  labs(x = NULL, y = NULL, fill = 'Correlation coefficient') + 
  theme_minimal()

```



```{=tex}
\pagebreak
\nocite{*}
\bibliography{CoronaSentiment}
\bibliographystyle{agsm}
\pagebreak
```

# Függelék: R kódok

```{r ref.label=setdiff(knitr::all_labels(), c("setup", "theme", "grid")), eval=FALSE, echo=T, attr.source='.numberLines'}
```
